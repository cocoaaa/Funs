{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cocoaaa/Funs/blob/master/Copy_of_ICRA24_tutorial_geometry_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl2nJSMF1669"
      },
      "source": [
        "# **Riemann and Gauss meet Asimov: 2nd Tutorial on Geometric Methods in Robot Learning, Optimization and Control** #\n",
        "# Riemannian optimization #\n",
        "---\n",
        "\n",
        "This Notebook aims at giving you a first glimpse of Riemannian optimization methods and of some of the existing Riemannian optimization libraries.\n",
        "\n",
        "In this notebook, we will see:\n",
        "*   How to implement a simple Riemannian gradient descent to optimize a controller gain on the SPD manifold,\n",
        "*   How to use some of the more complex Riemannian optimization methods from Pymanopt,\n",
        "*   How to use stochastic Riemannian optimizations approaches from Geoopt.\n",
        "\n",
        "The parts of the code that you should implement are indicated with the comment **# TODO** throughout the notebook.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVQn_D0oUuoX"
      },
      "source": [
        "# **1. Optimization of an impedance controller gain with Riemannian gradient descent**\n",
        "---\n",
        "In this first part, our goal is to learn the stiffness matrix of an impedance controller. We represent the movement of a planar robotâ€™s end-effector as\n",
        "a unit point mass moving in the plane $\\mathbb{R}^2$. The behavior of the end-effector is modelled as a spring-damper system: Given a desired position $\\hat{\\mathbf{x}}$, the end-effector acceleration is driven by the impedance controller $\\ddot{\\mathbf{x}} = \\mathbf{K}^p (\\hat{\\mathbf{x}} - \\mathbf{x}) - \\mathbf{K}^d \\dot{\\mathbf{x}} $, where $\\mathbf{K}^p$ and $\\mathbf{K}^d \\in\\mathcal{S}_{++}^D$ are the stiffness and damping gains, respectively.\n",
        "\n",
        "Given an demonstrated trajectory $\\{ \\mathbf{x}_t, \\dot{\\mathbf{x}}_t, \\ddot{\\mathbf{x}}_t \\}_{t=0}^T$, our goal is to infer the stiffness gain of our controller, such that the robot can reproduce the demonstrated trajectory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50zMBCL0IpG6"
      },
      "source": [
        "We start by installing and importing the required packages that we will use in the different examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjqLZnQRy0yG"
      },
      "source": [
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip install torch\n",
        "!pip install torchdiffeq\n",
        "!pip install jax\n",
        "!pip install pymanopt==2.0.1\n",
        "!pip install geoopt\n",
        "!pip install matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlaXAdKjI0E1"
      },
      "source": [
        "from typing import Union, Tuple\n",
        "import numpy as np\n",
        "import scipy.linalg\n",
        "import torch\n",
        "from torchdiffeq import odeint\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pl\n",
        "import matplotlib.path as mpath\n",
        "import matplotlib.patches as mpatches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define and display the demonstrated trajectory."
      ],
      "metadata": {
        "id": "fNdibbOY19bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrated trajectory\n",
        "# Position\n",
        "ref_traj_x_np = np.array([[-5.41421356, -3.41421356],\n",
        " [-5.26257245, -0.68248427],\n",
        " [-4.85836787,  1.31069968],\n",
        " [-4.37086438,  2.28727136],\n",
        " [-3.89674053,  2.7051812 ],\n",
        " [-3.47967017,  2.87399588],\n",
        " [-3.1331609,   2.94139824],\n",
        " [-2.85560442,  2.9692274 ],\n",
        " [-2.63883762,  2.98172984],\n",
        " [-2.47266588,  2.98810324]])\n",
        "ref_traj_x = torch.from_numpy(ref_traj_x_np)\n",
        "nb_points = ref_traj_x_np.shape[0]\n",
        "\n",
        "# Velocity\n",
        "ref_traj_dx_np = np.array([[0. ,        0.        ],\n",
        " [0.58141116, 3.70644107],\n",
        " [0.84598923, 2.30142563],\n",
        " [0.88546181, 1.07768499],\n",
        " [0.80934858, 0.45388672],\n",
        " [0.68813533, 0.1836573 ],\n",
        " [0.55969636, 0.07486522],\n",
        " [0.44187708, 0.03232233],\n",
        " [0.34149055, 0.01553348],\n",
        " [0.25969998, 0.00853142]])\n",
        "ref_traj_dx = torch.from_numpy(ref_traj_dx_np)\n",
        "\n",
        "# Acceleration\n",
        "ref_traj_ddx_np = np.array([[ 1.06568542e+00,  3.17296465e+01],\n",
        " [ 6.93707681e-01,  1.55189434e+00],\n",
        " [ 2.21675313e-01, -2.07292518e+00],\n",
        " [-6.33656592e-02, -1.43224769e+00],\n",
        " [-1.93525649e-01, -6.90170190e-01],\n",
        " [-2.32325929e-01, -2.92413406e-01],\n",
        " [-2.24805706e-01, -1.17028632e-01],\n",
        " [-1.97264018e-01, -4.61923722e-02],\n",
        " [-1.63734582e-01, -1.87786900e-02],\n",
        " [-1.31123463e-01, -8.27801519e-03]])\n",
        "ref_traj_ddx = torch.from_numpy(ref_traj_ddx_np)\n",
        "\n",
        "# Target\n",
        "x_target_np = ref_traj_x_np[-1]\n",
        "x_target = ref_traj_x[-1]\n",
        "\n",
        "# Initial conditions\n",
        "x0 = ref_traj_x[0]\n",
        "dx0 = ref_traj_dx[0]"
      ],
      "metadata": {
        "id": "DPOFAi9wPAQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "ax = plt.gca()\n",
        "plt.plot(ref_traj_x_np[:, 0], ref_traj_x_np[:, 1], color='seagreen', linewidth=2)\n",
        "for n in range(nb_points):\n",
        "    plt.plot(ref_traj_x_np[n, 0], ref_traj_x_np[n, 1], marker='.', markersize=10, color='seagreen')\n",
        "plt.plot(ref_traj_x_np[n, 0], ref_traj_x_np[n, 1], marker='*', markersize=15, color='seagreen')\n",
        "ax.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JN7gu3R4W06G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define an impedance controller\n",
        "\n",
        "$\\ddot{\\mathbf{x}} = \\mathbf{K}^p (\\hat{\\mathbf{x}} - \\mathbf{x}) - \\mathbf{K}^d \\dot{\\mathbf{x}}, $\n",
        "\n",
        "for our robot to reproduce the demonstrated trajectory.\n",
        "The stiffness gain $\\mathbf{K}^p$ is given as a parameter, and the damping gain $\\mathbf{K}^d$ is set so that the system is critically damped, i.e., $\\mathbf{K}^d = 2 \\sqrt{\\mathbf{K}^p}$.\n",
        "\n",
        "The resulting trajectory is then obtained by numerically integrating the acceleration $\\ddot{\\mathbf{x}}$ for $T$ timesteps."
      ],
      "metadata": {
        "id": "9XejMk9x2-b9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmuxP1nhXFa_"
      },
      "source": [
        "def point_mass_impedance_controller(x, dx, x_target, kp):\n",
        "    # Compute sqrtm of Kp\n",
        "    eigenvals, eigenvecs = torch.linalg.eigh(kp)\n",
        "    sqrtm_kp = torch.mm(torch.mm(eigenvecs, torch.diag(torch.sqrt(eigenvals))), eigenvecs.T)\n",
        "    # Compute the damping gain, such that the system is critically damped\n",
        "    kd = 2. * sqrtm_kp\n",
        "\n",
        "    # Acceleration = force / mass, with mass = 1.\n",
        "    acceleration = (torch.mm(kp, (x_target - x)[:, None]) - torch.mm(kd, dx[:, None]))[:, 0]\n",
        "    return acceleration\n",
        "\n",
        "def impedance_controller_ode(t, xdx, x_target, kp):\n",
        "    nb_dofs = int(xdx.shape[0] / 2.)\n",
        "    x = xdx[:nb_dofs]\n",
        "    dx = xdx[nb_dofs:]\n",
        "\n",
        "    dxddx = torch.zeros_like(xdx)\n",
        "    dxddx[:nb_dofs] = dx\n",
        "    dxddx[nb_dofs:] = point_mass_impedance_controller(x, dx, x_target, kp)\n",
        "\n",
        "    return dxddx\n",
        "\n",
        "def integrated_trajectory_ivp(x0, dx0, t, x_target, kp):\n",
        "    nb_dofs = x0.shape[0]\n",
        "    xdx0 = torch.hstack((x0, dx0))\n",
        "    res = odeint(lambda t, xdx: impedance_controller_ode(t, xdx, x_target, kp), xdx0, t, method='rk4')\n",
        "\n",
        "    return res[..., :nb_dofs], res[..., nb_dofs:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the influence of the stiffness gain on the reproduction, we draw some trajectories with different stiffness gains.\n",
        "\n",
        "Change the stiffness gain in the cell below and observe the resulting trajectory (blue) compared to the demonstration (green). Remember that the stiffness gain must be a symmetric positive definite 2x2 matrix."
      ],
      "metadata": {
        "id": "V165cRsp4ZjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the stiffness gain\n",
        "kp = np.array([[2.0, 0.], [0., .5]])  # TODO CHANGE\n",
        "\n",
        "# Reproduce trajectory\n",
        "t = torch.linspace(0., 5.0, nb_points)\n",
        "repro_x, _ = integrated_trajectory_ivp(x0, dx0, t, x_target, torch.from_numpy(kp))\n",
        "repro_x_np = repro_x.detach().numpy()\n",
        "\n",
        "# Plot reference and resulting trajectory with the given gain\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = plt.gca()\n",
        "plt.plot(ref_traj_x_np[:, 0], ref_traj_x_np[:, 1], color='seagreen', linewidth=2)\n",
        "plt.plot(repro_x_np[:, 0], repro_x_np[:, 1], color='navy', linewidth=2)\n",
        "for n in range(nb_points):\n",
        "    plt.plot(ref_traj_x_np[n, 0], ref_traj_x_np[n, 1], marker='.', markersize=10, color='seagreen')\n",
        "    plt.plot(repro_x_np[n, 0], repro_x_np[n, 1], marker='.', markersize=10, color='navy')\n",
        "plt.plot(ref_traj_x_np[n, 0], ref_traj_x_np[n, 1], marker='*', markersize=15, color='seagreen')\n",
        "ax.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "smkwa5oi5itR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now optimize the stiffness gain. To do so, we define our cost function as the norm of the residuals as in [1],\n",
        "\n",
        "$f(\\mathbf{K}^p) = \\| \\ddot{\\mathbf{x}}_t - \\mathbf{K}^p (\\hat{\\mathbf{x}} - \\mathbf{x}_t) + \\mathbf{K}^d \\dot{\\mathbf{x}}_t \\|$,\n",
        "\n",
        "where $\\{\\mathbf{x}_t, \\dot{\\mathbf{x}}_t, \\ddot{\\mathbf{x}}_t\\}$ are the demonstrated position, velocity, and acceleration, respectively.\n",
        "\n",
        "We define the stiffness gain estimation as the Riemannian optimization problem,\n",
        "\n",
        "$\\min_{\\mathbf{K}^p\\in\\mathcal{S}^2_{++}} f(\\mathbf{K}^p) $.\n",
        "\n",
        "\\\\\n",
        "\n",
        "[1] L. Rozo, S. Calinon, D. G. Caldwell, P. Jimnez, and C. Torras. **Learning Physical Collaborative Robot Behaviors From Human Demonstrations**, IEEE Trans. on Robotics 32, pp. 513â€“527, 2015."
      ],
      "metadata": {
        "id": "COzbdCaH4BKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first define the cost function $f(\\mathbf{K}^p)$ and the Euclidean gradient $\\text{grad}_{\\mathbb{R}} f(\\mathbf{K}^p)$."
      ],
      "metadata": {
        "id": "_3UI-T3jgFaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost(kp):\n",
        "  return np.linalg.norm(kp @ (x_target_np[:, None] - ref_traj_x_np.T) - ref_traj_ddx_np.T + kd @ ref_traj_dx_np.T)\n",
        "\n",
        "def euclidean_gradient(kp):\n",
        "  c = cost(kp)\n",
        "  return (kp @ (x_target_np[:, None] - ref_traj_x_np.T) - ref_traj_ddx_np.T + kd @ ref_traj_dx_np.T) @ (x_target_np[None] - ref_traj_x_np) / c"
      ],
      "metadata": {
        "id": "WxdYgZy8qDud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define utility functions for the SPD manifold:\n",
        "- The exponential map;\n",
        "- The Riemannian gradient, computed from the Euclidean gradient."
      ],
      "metadata": {
        "id": "mgIwhVuhrXgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spd_expmap(point, tangent_vector):\n",
        "  p_inv_tv = np.linalg.solve(point, tangent_vector)\n",
        "  return point @ scipy.linalg.expm(p_inv_tv)\n",
        "\n",
        "def spd_euclidean_to_riemannian_gradient(point, euclidean_gradient):\n",
        "  return 0.5 * point @ (euclidean_gradient + euclidean_gradient.T) @ point"
      ],
      "metadata": {
        "id": "3Mlo8kDGgMc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now solve the optimization $\\min_{\\mathbf{K}^p\\in\\mathcal{S}^2_{++}} f(\\mathbf{K}^p) $ via Riemannian gradient. Implement the missing parts of the Riemannian gradient descent in the cell below. Then, observe the evolution of the cost, and gradient norm, as well as the resuling optimized stiffness."
      ],
      "metadata": {
        "id": "hU3So_zvrwXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "K = 30  # Number of iterations\n",
        "step_size = 0.05\n",
        "kd = np.array([[1.0, 0.0], [0.0, 1.0]])  # Constant estimate of the damping gain\n",
        "\n",
        "# Initialization\n",
        "kp_k = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
        "\n",
        "# Riemannian gradient descent\n",
        "print('Iteration\\t', 'Cost\\t\\t\\t', 'Norm gradient')\n",
        "print('---------\\t', '------------------\\t', '------------------')\n",
        "for k in range(K):\n",
        "  # Compute the Riemannian gradient\n",
        "  g_k = ...  # TODO implement\n",
        "  # Update step\n",
        "  kp_k = ... # TODO implement\n",
        "\n",
        "  # Compute current cost\n",
        "  c_k = cost(kp_k)\n",
        "  print(k, '\\t\\t', c_k, '\\t', np.linalg.norm(g_k))\n",
        "\n",
        "# Optimized gain\n",
        "kp_opt = kp_k\n",
        "print(\"\\n\\n Optimal Kp:\\n\", kp_opt)"
      ],
      "metadata": {
        "id": "ct_oQQLVijCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We reproduce the trajectory with the optimal stiffness gain."
      ],
      "metadata": {
        "id": "FCVc-faPsJdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproduce trajectory\n",
        "repro_x, _ = integrated_trajectory_ivp(x0, dx0, t, x_target, torch.from_numpy(kp_opt))\n",
        "repro_x_np = repro_x.detach().numpy()\n",
        "\n",
        "# Plot reference and resulting trajectory after optimizing the impedance gain\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = plt.gca()\n",
        "plt.plot(ref_traj_x_np[:, 0], ref_traj_x_np[:, 1], color='seagreen', linewidth=2)\n",
        "plt.plot(repro_x_np[:, 0], repro_x_np[:, 1], color='navy', linewidth=2)\n",
        "for n in range(nb_points):\n",
        "    plt.plot(ref_traj_x_np[n, 0], ref_traj_x_np[n, 1], marker='.', markersize=10, color='seagreen')\n",
        "    plt.plot(repro_x_np[n, 0], repro_x_np[n, 1], marker='.', markersize=10, color='navy')\n",
        "plt.plot(ref_traj_x_np[n, 0], ref_traj_x_np[n, 1], marker='*', markersize=15, color='seagreen')\n",
        "ax.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y31zdsi2qzhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Optimization of an impedance controller gain with Pymanopt**\n",
        "---\n",
        "In this second part, we leverage some of the more complex Riemannian optimization methods implemented in the library Pymanopt to optimize our stiffness gain.\n",
        "\n",
        "Pymanopt is used to optimize a cost function by executing the following steps:\n",
        "- Instantiate the manifold on which our optimized variable lies;\n",
        "- Define our cost function;\n",
        "- Create a Pymanopt problem;\n",
        "- Instantiate the Riemannian optimizer of our choice;\n",
        "- Run the optimizer.\n",
        "\n",
        "These different steps are implemented below and several solvers are imported. Take the time to try them out and to observe their evolution and the resuling optimized stiffness as dislayed after the cell."
      ],
      "metadata": {
        "id": "dsrahPdsXKdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pymanopt\n",
        "import pymanopt\n",
        "from pymanopt import Problem\n",
        "from pymanopt.manifolds import SymmetricPositiveDefinite\n",
        "from pymanopt.optimizers import ConjugateGradient, SteepestDescent, TrustRegions"
      ],
      "metadata": {
        "id": "qm9c_rEPXh0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create SPD manifold of dimension 2 (2x2 SPD matrices)\n",
        "manifold = SymmetricPositiveDefinite(2)\n",
        "\n",
        "# Create pymanopt cost function\n",
        "# The cost function must be decorated with a backend decorator. Here we use torch.\n",
        "@pymanopt.function.pytorch(manifold)\n",
        "def cost(kp):\n",
        "  # Constant estimate of the damping gain\n",
        "  kd = torch.from_numpy(np.array([[1.0, 0.0], [0.0, 1.0]]))\n",
        "  # Cost\n",
        "  return torch.norm(torch.mm(kp, (x_target - ref_traj_x).T) - ref_traj_ddx.T + torch.mm(kd, ref_traj_dx.T))\n",
        "\n",
        "# Create pymanopt problem\n",
        "problem = Problem(manifold=manifold, cost=cost)\n",
        "\n",
        "# Instantiate optimizer\n",
        "optimizer = ConjugateGradient()  # TODO CHANGE OPTIMIZER\n",
        "\n",
        "# Optimize\n",
        "result = optimizer.run(problem)\n",
        "kp_opt = result.point\n",
        "kp_opt_cost = result.cost\n",
        "\n",
        "# Optimized gain\n",
        "print(\"Optimal Kp:\\n\", kp_opt)"
      ],
      "metadata": {
        "id": "Fa5ToaHm7vR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We reproduce the trajectory with the optimal stiffness gain.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yAzSPgHT7hmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproduce trajectory\n",
        "repro_x, _ = integrated_trajectory_ivp(x0, dx0, t, x_target, torch.from_numpy(kp_opt))\n",
        "repro_x_np = repro_x.detach().numpy()\n",
        "\n",
        "# Plot reference and resulting trajectory after optimizing the impedance gain\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = plt.gca()\n",
        "plt.plot(ref_traj_x_np[:, 0], ref_traj_x_np[:, 1], color='seagreen', linewidth=2)\n",
        "plt.plot(repro_x_np[:, 0], repro_x_np[:, 1], color='navy', linewidth=2)\n",
        "for n in range(nb_points):\n",
        "    plt.plot(ref_traj_x_np[n, 0], ref_traj_x_np[n, 1], marker='.', markersize=10, color='seagreen')\n",
        "    plt.plot(repro_x_np[n, 0], repro_x_np[n, 1], marker='.', markersize=10, color='navy')\n",
        "plt.plot(ref_traj_x_np[n, 0], ref_traj_x_np[n, 1], marker='*', markersize=15, color='seagreen')\n",
        "ax.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OkdxQ2jb7y5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Optimization of an impedance controller gain with Geoopt**\n",
        "---\n",
        "In this third part, we leverage stochastic Riemannian optimization methods implemented in the library Geoopt to optimize our stiffness gain.\n",
        "\n",
        "Geoopt is used to optimize a cost function by executing the following steps:\n",
        "- Define our cost function;\n",
        "- Instantiate the manifold on which our optimized variable lies;\n",
        "- Initialize the optimization variable as a Geoopt manifold parameter;\n",
        "- Instantiate the Riemannian stochastic optimizer of our choice;\n",
        "- Run the optimizer as a Pytorch optim.\n",
        "\n",
        "These different steps are implemented below and several solvers are imported. Take the time to try them out and to observe their evolution and the resuling optimized stiffness as dislayed after the cell."
      ],
      "metadata": {
        "id": "wE5_YVpLufCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from geoopt.manifolds import SymmetricPositiveDefinite\n",
        "from geoopt import ManifoldParameter, ManifoldTensor\n",
        "from geoopt.optim import RiemannianAdam, RiemannianSGD\n"
      ],
      "metadata": {
        "id": "OidAMxc9ud5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost(kp):\n",
        "  # Constant estimate of the damping gain\n",
        "  kd = torch.from_numpy(np.array([[1.0, 0.0], [0.0, 1.0]]))\n",
        "  # Cost\n",
        "  return torch.norm(torch.mm(kp, (x_target - ref_traj_x).T) - ref_traj_ddx.T + torch.mm(kd, ref_traj_dx.T))"
      ],
      "metadata": {
        "id": "4E6oAsXkvQxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "K = 100  # Number of iterations\n",
        "lr = 1e-2\n",
        "\n",
        "# Instantiate the manifold\n",
        "manifold = SymmetricPositiveDefinite()\n",
        "\n",
        "# Initialization of Kp as a Manifold parameter\n",
        "kp = ManifoldParameter(torch.from_numpy(np.eye(2)), manifold=manifold)\n",
        "\n",
        "# Instanciate the optimizer\n",
        "optimizer = RiemannianAdam([kp], lr=1e-2)  # TODO CHANGE OPTIMIZER\n",
        "\n",
        "# Riemannian gradient descent\n",
        "print('Iteration\\t', 'Cost\\t\\t\\t')\n",
        "print('---------\\t', '------------------\\t')\n",
        "for k in range(K):\n",
        "  loss = cost(kp)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print(k, '\\t\\t', loss.detach().numpy())\n",
        "\n",
        "# Optimized gain\n",
        "kp_opt = kp\n",
        "print(\"\\n\\n Optimal Kp:\\n\", kp_opt.detach().numpy())"
      ],
      "metadata": {
        "id": "0A5kPbvrvzq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We reproduce the trajectory with the optimal stiffness gain."
      ],
      "metadata": {
        "id": "FZZ9uM4OyYPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproduce trajectory\n",
        "repro_x, _ = integrated_trajectory_ivp(x0, dx0, t, x_target, kp_opt)\n",
        "repro_x_np = repro_x.detach().numpy()\n",
        "\n",
        "# Plot reference and resulting trajectory after optimizing the impedance gain\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = plt.gca()\n",
        "plt.plot(ref_traj_x_np[:, 0], ref_traj_x_np[:, 1], color='seagreen', linewidth=2)\n",
        "plt.plot(repro_x_np[:, 0], repro_x_np[:, 1], color='navy', linewidth=2)\n",
        "for n in range(nb_points):\n",
        "    plt.plot(ref_traj_x_np[n, 0], ref_traj_x_np[n, 1], marker='.', markersize=10, color='seagreen')\n",
        "    plt.plot(repro_x_np[n, 0], repro_x_np[n, 1], marker='.', markersize=10, color='navy')\n",
        "plt.plot(ref_traj_x_np[n, 0], ref_traj_x_np[n, 1], marker='*', markersize=15, color='seagreen')\n",
        "ax.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vr5U_3_OyYWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}